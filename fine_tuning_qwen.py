# -*- coding: utf-8 -*-
"""Fine_tuning_qwen.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1m8ef5V_Ymff6R5XpIHNc5AoD-luWJ5ns
"""

from google.colab import drive
drive.mount('/content/drive')

from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline

import torch

model_path = "/content/drive/MyDrive/HuggingFace_Model/Qwen1.5-0.5B-Chat"

import os
print("Files in folder:")
for file in sorted(os.listdir(model_path)):
    print("  ", file)

model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16)
model

tokenizer = AutoTokenizer.from_pretrained(model_path)
tokenizer

# Use the Qwen chat template correctly
messages = [
    {"role": "system", "content": "You are a helpful AI assistant."},
    {"role": "user", "content": "Hello, who are you?"}
]

prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

# Generate response
outputs = model.generate(**inputs, max_new_tokens=100)
response = tokenizer.decode(outputs[0], skip_special_tokens=True)

print(response)

from datasets import load_dataset

data_set_path = "/content/drive/MyDrive/Fine_Tuning_Data/people_data.json"
raw_data = load_dataset("json", data_files=data_set_path)

def flatten(example):
    resp = example["response"]
    completion = f"name: {resp['name']}, age: {resp['age']}, job: {resp['job']}, gender: {resp['gender']}"
    return {
        "prompt": example["prompt"],
        "completion": completion
    }

processed = raw_data["train"].map(flatten, remove_columns=raw_data["train"].column_names)

dataset = processed.train_test_split(test_size=0.25, seed=42)

print("Train example:")
print(dataset["train"][1])

print("\nTest example:")
print(dataset["test"][1])

print("Train size:", len(dataset["train"]))
print("Test size:", len(dataset["test"]))

dataset['train']['prompt'][1]

text = dataset['train']['prompt'][1] + "\n" + dataset['train']['completion'][1]
print(text)



tokens = tokenizer(
    text,
    max_length=128,
    truncation=True,
    padding="max_length"
)
tokens

tokenizer = AutoTokenizer.from_pretrained(model_path)

def preprocess(sample):
    sample = sample["prompt"] + "\n" + sample["completion"]

    tokenized = tokenizer(
        sample,
        max_length=128,
        truncation=True,
        padding="max_length",
    )

    tokenized["labels"] = tokenized["input_ids"].copy()
    return tokenized

data = dataset.map(preprocess)

print(data["train"][0])

print(data["test"][15])

from peft import LoraConfig, get_peft_model, TaskType
from transformers import AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained(
    model_path,
    device_map="auto",
    torch_dtype=torch.float16
)

lora_config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,
    target_modules=["q_proj", "k_proj", "v_proj"],
    r=8,
    lora_alpha=16,
    lora_dropout=0.01
)

model = get_peft_model(model, lora_config)

model.print_trainable_parameters()

# Commented out IPython magic to ensure Python compatibility.
# %pip install evaluate

from transformers import TrainingArguments, Trainer
import evaluate

training_args = TrainingArguments(
    num_train_epochs=7,
    learning_rate=0.001,
    #per_device_train_batch_size=2,
    #per_device_eval_batch_size=2,
    #evaluation_strategy="epoch",
    #save_strategy="epoch",
    logging_steps=25
    #output_dir="./results",
    #fp16=True
)

metric = evaluate.load("accuracy")
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = logits.argmax(-1)
    true = labels[labels != -100]
    pred = preds[labels != -100]
    return metric.compute(predictions=pred, references=true)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=data["train"],
    eval_dataset=data["test"],
    compute_metrics=compute_metrics
)

trainer.train()

eval_results = trainer.evaluate()
print("Eval results:", eval_results)

prompt = "Under the scorching summer sun, Maya works as a biologist. She is known among friends for learning sign language in quiet solitude."

inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
outputs = model.generate(**inputs, max_new_tokens=128)
print("Generated:", tokenizer.decode(outputs[0], skip_special_tokens=True))

print(data["test"][17])

from tqdm import tqdm

def evaluate_model(model, tokenizer, dataset, num_samples=20, max_new_tokens=64):
    correct = 0
    total = 0

    for i in tqdm(range(num_samples)):
        example = dataset[i]
        prompt = example["prompt"]
        true_completion = example["completion"]

        # توليد من الموديل
        inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
        outputs = model.generate(**inputs, max_new_tokens=max_new_tokens)
        pred = tokenizer.decode(outputs[0], skip_special_tokens=True)

        # نجيب آخر جزء (اللي فيه completion)
        if "name:" in pred:
            pred_completion = pred.split("name:")[-1].strip()
            pred_completion = "name: " + pred_completion
        else:
            pred_completion = pred.strip()

        # مقارنة (بسيطة: string match)
        if pred_completion == true_completion:
            correct += 1
        total += 1

        print(f"\nPrompt: {prompt}")
        print(f"True: {true_completion}")
        print(f"Pred: {pred_completion}")

    acc = correct / total if total > 0 else 0
    print(f"\n✅ Accuracy on {num_samples} samples = {acc:.2f}")
    return acc

acc = evaluate_model(model, tokenizer, dataset["test"], num_samples=10)